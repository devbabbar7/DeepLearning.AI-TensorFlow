{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devbabbar7/DeepLearning.AI-TensorFlow/blob/main/Natural%20Language%20Processing%20Tensorflow/Tokenizer_Padding_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer Basics\n",
        "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your corpus (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called tokens and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells.\n",
        "\n",
        "Generating the vocabulary\n",
        "In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the fit_on_texts() method and you can get the result by looking at the word_index property. More frequent words have a lower index."
      ],
      "metadata": {
        "id": "D42WaGLwZjCR"
      },
      "id": "D42WaGLwZjCR"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat'\n",
        "    ]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# Sentences with tokens\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W317-JkDZkb9",
        "outputId": "fa23f254-f53a-4df8-c2cd-f0cf9e043662"
      },
      "id": "W317-JkDZkb9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n",
            "[[1, 2, 3, 4], [1, 2, 3, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# Sentences with tokens\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMPvsNNkZ7A1",
        "outputId": "1e3e8f45-2bc9-4218-feb1-ac621a27723b"
      },
      "id": "CMPvsNNkZ7A1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n",
            "[[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=10)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nPadded Sequences 2:\")\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZ9JMxAcOgY",
        "outputId": "013f75a9-2199-4e0b-b90f-433b4b62febb"
      },
      "id": "GWZ9JMxAcOgY",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Sequences:\n",
            "[[0 3 1 2 4]\n",
            " [0 3 1 2 5]\n",
            " [0 6 1 2 4]]\n",
            "\n",
            "Padded Sequences 2:\n",
            "[[0 0 0 0 0 0 3 1 2 4]\n",
            " [0 0 0 0 0 0 3 1 2 5]\n",
            " [0 0 0 0 0 0 6 1 2 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Out-of-vocabulary tokens\n",
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "# Generate the sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Print the word index dictionary\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "\n",
        "# Print the sequences with OOV\n",
        "print(\"\\nTest Sequence = \", test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA5Y4d6sZ7QG",
        "outputId": "aa9ed168-1ba0-4408-986f-1782d79975f3"
      },
      "id": "iA5Y4d6sZ7QG",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n",
            "\n",
            "Test Sequence =  [[3, 1, 2, 4], [2, 4, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the test is ignoring the words with no tokenization."
      ],
      "metadata": {
        "id": "BY9zSSwib0ei"
      },
      "id": "BY9zSSwib0ei"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OOV_TOKEN\n",
        "In natural language processing, OOV stands for \"Out-of-Vocabulary.\" An OOV token is a special token that is used by tokenizers to represent any word or character that is not present in the tokenizer's vocabulary."
      ],
      "metadata": {
        "id": "bE9XA7-mfPdY"
      },
      "id": "bE9XA7-mfPdY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>')\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# Sentences with tokens\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n",
        "\n",
        "\n",
        "#Out-of-vocabulary tokens\n",
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "# Generate the sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Print the word index dictionary\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "\n",
        "# Print the sequences with OOV\n",
        "print(\"\\nTest Sequence = \", test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiCgKAc-bxqA",
        "outputId": "310dd5f0-6bc1-410f-d5d7-26ac27847850"
      },
      "id": "MiCgKAc-bxqA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n",
            "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n",
            "\n",
            "Word Index =  {'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n",
            "\n",
            "Test Sequence =  [[4, 1, 2, 3, 5], [3, 5, 1, 3, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TpIaU8QfK3Q"
      },
      "id": "1TpIaU8QfK3Q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}